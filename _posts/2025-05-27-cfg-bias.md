---
layout: distill
title: How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?
description: Classifier-Free Guidance (CFG) is a potential driver of bias amplification in text-to-image (T2I) diffusion models. We analyzed the magnitude and pattern of CFG-induced bias
amplification.
tags: CFG diffusion_model bias
giscus_comments: false
date: 2025-05-27
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Myeongsoo Kim
    url: "https://ime.postech.ac.kr/"
    affiliations:
      name: Industrial Management and Engineering, POSTECH
  - name: Kyungmin Kim
    url: "https://ai.postech.ac.kr/"
    affiliations:
      name: Graduate School of Artificial Intelligence, POSTECH
  # - name: Nathan Rosen
  #   url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
  #   affiliations:
  #     name: IAS, Princeton

bibliography: 2025-05-27-cfg-bias.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    subsections:
      - name: Motivation
  - name: Preliminary
    subsections:
      - name: Group Fairness and Bias amplification
      - name: Diffusion Model and Classifier-Free guidance
  - name: Experiment
    subsections:
      - name: T2I diffusion model : Stable diffusion
      - name: Class conditional diffusion model : Colored MNIST
  - name: Discussion

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---
## Introduction

### Motivation

Text-to-image (T2I) diffusion models have remarkable capabilities in generating realistic images from input text. <d-cite key="seshadri2024bias"></d-cite> However, there are growing concerns about their tendency to produce biased images toward specific demographic groups e.g. gender bias, skin tone bias, attribute bias, …. Because diffusion models learn from large-scale datasets that often encode social biases, they tend to generate images biased toward specific demographic groups. We address the bias amplification phenomenon such that a model’s outputs exhibit stronger biases than those present in its training data like figure 1.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/ba_figure.png"
       title="Figure 1. The Bias Amplification Paradox in Text-to-Image Generation <d-cite key="seshadri2024bias"></d-cite>"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

Which factors drive bias amplification in T2I diffusion models? We pinpoint Classifier-Free Guidance (CFG)—universally applied during T2I diffusion model generation—as a potential contributor. While CFG substantially improves image fidelity and alignment with the conditioning prompt, elevating its strength inherently diminishes sample diversity and biases generation toward prototypical outputs. In practice, users typically operate in the higher guidance‐scale range, adjusting the parameter to obtain their desired images.

Recently, <d-cite key="kim2024rethinking"></d-cite> empirically demonstrated that higher CFG scales correspond to an increased prevalence of majority-group attributes, thereby identifying CFG intensity as a direct mechanism of bias amplification.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/cfg_ba.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation <d-cite key="seshadri2024bias"></d-cite>" 
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

In this blog post, we analyze how Classifier‐Free Guidance (CFG) drives bias amplification in text‐to‐image diffusion models. Our work proceeds in three steps: (1) we formulate the problem by surveying prior research on CFG’s impact on model distributions; (2) we quantitatively assess the degree to which CFG contributes to bias amplification; and (3) we characterize the relationship between guidance scale and bias trends. We conducted our experimental analysis on the gender–occupation bias, a primary focus in fairness studies of text-to-image generative models.

## Preliminary

### Group Fairness and Bias amplification

### Diffusion Model and Classifier-free guidance

Recent diffusion models belong to the family of score-based generative models and are formulated within the stochastic differential equation (SDE) framework.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/sde.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

The forward process refers to perturbing the data distribution via a diffusion process. As indicated by the symmetric coloring of the density-representing boxes in the figure, the reverse process reconstructs the marginal distributions of the forward process at each time step. To generate images, one must solve the reverse SDE, which therefore requires an accurate estimate of the score function.

Classifier‐Free Guidance (CFG) is a sampling strategy that employs an extrapolated score toward the conditional distribution, controlled by the guidance scale w, as follows. It pushes samples in the direction of higher $p(y \mid x)$, but it does not directly correspond to the distribution $p_w$.

p

Here, it is important to note that the sampling distribution has deviated from the original data distribution. Bias amplification refers to the phenomenon in which a diffusion model’s sample distribution—expected in theory to follow the data distribution—actually exhibits an exacerbated bias. By applying CFG, however, the intended sampling distribution is inherently shifted from the pure data distribution to one weighted by both p(y∣x) and the guidance scale w.

Furthermore, we demonstrated in a simple setup (see Figure 4) that this shift acts to amplify bias in gender-occupation fairness. While the unconditional distribution corresponds to a single marginal distribution, the conditional distribution can exhibit higher or lower densities for specific groups depending on the conditioning variable y.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/gmm.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

In the Experiments section, we empirically demonstrate that this tendency can indeed be observed in both text-to-image diffusion models and a toy example.

---

## Experiment

### T2I diffusion model : Stable diffusion

#### Setting

We tested our hypothesis using the experimental setup of <d-cite key="seshadri2024bias"></d-cite>. The main procedure is as follows: we generate sets of images with Stable Diffusion v1.5 conditioned on prompts that include each target occupation. We then use a CLIP-based classifier to measure the gender distribution of the generated images and compare it against the gender proportions in the LAION training data. This process is repeated for each of the occupations listed below, varying the guidance scale 1~15 to observe how bias changes.

- Occupation list : **skew to Male**(’manager’, ‘scientist’, ‘poet’), **fairly balanced**(’dentist’, ‘painter’, ‘pharmacist’), **skew to female** (’dancer’, ‘hairdresser’, ‘pharmacist’)

#### Gender distribution and Bias amplification

<div class="row justify-content-sm-center">
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/sd_plot1.png"
       title="왼쪽 이미지 캡션"
       class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/sd_plot2.png"
       title="오른쪽 이미지 캡션"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

In Figures A and B, we observe that for certain occupations, increasing the guidance scale drives the gender ratio further away from the balanced 0.5 mark, illustrating bias amplification. This effect is even more pronounced when viewed in each prompt’s individual plot, confirming that part of the previously reported bias amplification arises from CFG.

Furthermore, in Figure A, using a baseline female ratio of approximately 0.3–0.4 as a reference, we find that occupations with higher initial female proportions see further increases in female representation as guidance scale grows, whereas occupations with lower initial female proportions experience declines. This suggests that this corresponds to the marginal female proportion in the distribution, which is approximately 0.34—the average female ratio in Stable Diffusion reported by <d-cite key="cho2023dall"></d-cite>.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/dalleval.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

#### Bias amplifaction through Guidance

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/sd_plot3.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

Therefore, to quantify the extent to which bias is amplified relative to the unconditional model’s gender distribution (female ratio = 0.34), we plotted Figure C using the following metric. We observe that the bias amplification values are positive for most occupations and tend to increase as the guidance scale grows, with the majority of the change occurring at lower guidance scales.

### Class conditional diffusion model

In this section, we replicate and validate the bias amplification trends observed in text-to-image diffusion models within a small-scale, class-conditional diffusion setting. We investigated gender–occupation bias by varying the red/green ratio for each digit in the Colored MNIST dataset. Therefore, digits can be viewed as occupations, and colors as genders.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/te_dt.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

When we examined the bias amplification trend using a diffusion model trained on this dataset via the same procedure, the results (Figure #) showed clear amplification from the unconditional color ratio of 5:5—deviations further from the conditional distribution produced stronger amplification. 

<div class="row justify-content-sm-center">
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/te_plot1.png"
       title="왼쪽 이미지 캡션"
       class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/te_plot2.png"
       title="오른쪽 이미지 캡션"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

We observed the same pattern under Classifier Guidance, and when we reversed the color ratios during dataset synthesis, the direction of the bias change also reversed. This illustrates that the conditional distribution p(y∣x)p(y \mid x) used for guidance exerts a significant influence on the distribution of guided diffusion models, depending on the guidance scale.

<div class="row justify-content-sm-center">
  <div class="col-sm-10 mt-3 mt-md-0">
    {% include figure.liquid
       path="assets/img/cfg_ba/te_plot3.png"
       title="Figure 2. The Bias Amplification Paradox in Text-to-Image Generation"
       class="img-fluid rounded z-depth-1" %}
  </div>
</div>

---

## Discussion

In this blog post, we empirically assessed the impact of CFG on bias amplification in T2I diffusion models, demonstrating that CFG amplifies bias by extrapolating 

the conditional distribution further away from the unconditional distribution.

The limitations not addressed in this post are as follows:

- The numerical values cannot be considered strictly precise because our evaluation operates on high-dimensional, complex text-conditioned distributions.
- Images generated at low guidance scales often fail to form properly, making those measurements less reliable. Since we assigned gender by mapping each image to the closest class using a CLIP classifier, the CLIP model’s own biases may have influenced our results.

To address these, we validated the observed trends using toy examples.

Through this blog post, we emphasize that group-fairness research for diffusion models must address the effects of CFG, and more broadly, that debiasing low-temperature sampling strategies is essential for generative models. 

- In most of the papers I reviewed, there is little to no discussion of CFG’s impact, and their experimental setups are often missed.
- Kim et al. (2024), who previously identified the same problem, proposed a solution that attenuates bias in the text-condition embeddings of the text-conditional component.  We approached the problem from a slightly different angle, framing it as the disparity in bias levels for the attribute between the conditional distribution and the unconditional distribution.
- Our proposal is to reduce this disparity, and we believe that one promising direction is to leverage Autoguidance—one of the recent state-of-the-art CFG methods. This methodology leverages the conditional distribution of a weaker model rather than the unconditional distribution. However, it still does not resolve which weaker model should be employed within T2I diffusion frameworks, necessitating further research.

We have always considered fairness only with respect to $p(x \mid y)$, but for practical applications, fairness in $p(y \mid x)$ must also be taken into account.

---