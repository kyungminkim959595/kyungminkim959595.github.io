---
layout: distill
title: How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?
description: Literature review and empirical analyses on the bias amplification in CFG for diffusion models
tags: CFG diffusion_model bias
giscus_comments: true
date: 2025-05-27
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Myeongsoo Kim
    url: "https://ime.postech.ac.kr/"
    affiliations:
      name: Industrial Management and Engineering, POSTECH
  - name: Kyungmin Kim
    url: "https://ai.postech.ac.kr/"
    affiliations:
      name: Graduate School of Artificial Intelligence, POSTECH
  # - name: Nathan Rosen
  #   url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
  #   affiliations:
  #     name: IAS, Princeton

bibliography: 2025-05-27-cfg-bias.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Group Fairness and Bias Amplification in Generative Models
    # if a section has subsections, you can add them as follows:
    subsections:
      - name: Group Fairness in Generative Models
      - name: Bias Amplification in Generative Models
      - name: Demographic Bias in Text-to-image Diffusion Models
      - name: Bias Evaluation in Text-to-image Diffusion Models
  - name: Bias Ampliification in Classifier-free Guidance for Diffusion Models

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Group Fairness and Bias Amplification in Generative Models

### Group Fairness in Generative Models

In the context of generative models, group fairness generally refers to ensuring that the model's outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition: 

$$
  \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right)
  \approx
  \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),
$$

for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model's output distribution is not biased toward any specific sub-population.

### Bias Amplification in Generative Models

Bias amplification is the phenomenon where a model's outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.

### Demographic Bias in Text-to-image Diffusion Models

There has been a surge of empirical findings demonstrating demographic bias in text-to-image diffusion models. Most studies curate and utilize diagnostic prompts to systmetically analyze bias in state-of-the-art models such as DALL-E and Stable Diffusion. Although the specific image generation task and the protected attribute may vary, the majority of studies aim to determine whether bias persists even when neutral prompts are used. Specifically, they examine whether certain occupations or adjectives are more freqeuntly associated with sub-populations defined by demographic attributes. For example, <d-cite key="wu2024stable"></d-cite> showed that prompts containing positive adjectives are more likely to generate images of lighter skin subjects, while prompts containing negative attributes tend to produce images of darker-skinned subjects. In addition to such distributional bias across different demographic groups, bias can also be perpetuated in terms of generation diversity: <d-cite key="aldahoul2025ai"></d-cite> showed that Stable Diffusion XL tends to generate similar-looking faces for minority groups, without catching any details and nuances of their facial expressions.

### Bias Evaluation in Text-to-image Diffusion Models
In addition to the curation of the diagnostic prompts, it is essential to annotate each generated image with both its corresponding label and associated protected attribute. To evaluate the bias of a given text-to-image diffusion model, we first generate a large sample of images for each of the prompt. For example, in the occupation image generation task, we prompt the model with $``\text{Image of a} ~ \texttt{\{ job \}}''$ to generate multiple samples. Then, either a human annotator or a model is used to annotate the protected attribute for each image in order to evaluate group fairness. While the literature still lacks a unified benchmark and evaluation framework for assessing bias in text-to-image diffusion models, existing studies consistently report that such bias exists and must be mitigated to ensure the trustworthy use of these models.

---

## Bias Amplification in Classifier-free Guidance for Diffusion Models

---