---
layout: distill
title: How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?
description: Literature review and empirical analyses on the bias amplification in CFG for diffusion models
tags: CFG diffusion_model bias
giscus_comments: false
date: 2025-05-27
featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Myeongsoo Kim
    url: "https://ime.postech.ac.kr/"
    affiliations:
      name: Industrial Management and Engineering, POSTECH
  - name: Kyungmin Kim
    url: "https://ai.postech.ac.kr/"
    affiliations:
      name: Graduate School of Artificial Intelligence, POSTECH
  # - name: Nathan Rosen
  #   url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
  #   affiliations:
  #     name: IAS, Princeton

bibliography: 2025-05-27-cfg-bias.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Group Fairness and Bias Amplification in Generative Models
    # if a section has subsections, you can add them as follows:
    subsections:
      - name: Group Fairness in Generative Models
      - name: Bias Amplification in Generative Models
      - name: Demographic Bias in Text-to-image Diffusion Models
      - name: Bias Evaluation in Text-to-image Diffusion Models
  - name: Bias Ampliification in Classifier-free Guidance for Diffusion Models

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Group Fairness and Bias Amplification in Generative Models

### Group Fairness in Generative Models

In the context of generative models, group fairness generally refers to ensuring that the model's outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition: 

$$
  \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right)
  \approx
  \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),
$$

for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model's output distribution is not biased toward any specific sub-population.

### Bias Amplification in Generative Models

Bias amplification is the phenomenon where a model's outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.

### Demographic Bias in Text-to-image Diffusion Models

There has been a surge of empirical findings demonstrating demographic bias in text-to-image diffusion models. Most studies curate and utilize diagnostic prompts to systmetically analyze bias in state-of-the-art models such as DALL-E and Stable Diffusion. Although the specific image generation task and the protected attribute may vary, the majority of studies aim to determine whether bias persists even when neutral prompts are used. Specifically, they examine whether certain occupations or adjectives are more freqeuntly associated with sub-populations defined by demographic attributes. For example, <d-cite key="wu2024stable"></d-cite> showed that prompts containing positive adjectives are more likely to generate images of lighter skin subjects, while prompts containing negative attributes tend to produce images of darker-skinned subjects. In addition to such distributional bias across different demographic groups, bias can also be perpetuated in terms of generation diversity: <d-cite key="aldahoul2025ai"></d-cite> showed that Stable Diffusion XL tends to generate similar-looking faces for minority groups, without catching any details and nuances of their facial expressions.

### Bias Evaluation in Text-to-image Diffusion Models

In addition to curation of the diagnostic prompts, it is crucial to annotate each generated image with both its corresponding label and protected attribute. To assess bias in a text-to-image diffusion model, we begin by generating a large sample of images for each prompt. For example, in the occupation image generation task, we prompt the model with "$\text{Image of a} ~ \texttt{\{job\}}$" to generate multiple samples. Then, either a human annotator or a model is used to annotate the protected attribute of each image, allowing for the evaluation of group fairness. Demographic parity across gender, for instance, can then be measured by computing the proportion of male and female images generated for each occupation. Although there is still no unified benchmark and evaluation framework for bias assessment in text-to-image diffusion models, existing studies consistently report that such biases exist and must be mitigated to ensure the trustworthy use of these models.

---

## Bias Amplification in Classifier-free Guidance for Diffusion Models

While the classifier-free guidance in image generation helps text-to-image diffusion models produce images that more faithfully align with the prompt semantics, this alignment can also lead to bias amplification. Specifically, the guidance scale can be seen as a double-edged sword, as it not only aligns the output more closely with the prompt, but also acts as an amplifier of the biases the model has learned from its training data. Recently, <d-cite key="kim2024rethinking"></d-cite> empirically demonstrated how varying the guidance scale impacts bias amplification. Lower guidance tends to yield more balanced outputs that better reflect the distribution in the training data, albeit with relatively low fidelity. Meanwhile, <d-cite key="kim2024rethinking"></d-cite> has additionally reported that it is much more feasible to produce minority outcomes from the latent space, which could have been difficult to sample in the original space. While several studies have explored ways to mitigate bias in classifier-free guidance, for example, such as prepending fairness prompts <d-cite key="friedrich2023fair"></d-cite> or applying fine-tuning <d-cite key="shen2024finetuning"></d-cite>, we focus on the detailed analysis of how the guidance scale affects the bias amplification on text-to-image diffusion models in subsequent sections.

---