<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models? | Efficient ML Systems (EECE695E) Blog Post </title> <meta name="author" content="You R. Name"> <meta name="description" content="Classifier-Free Guidance (CFG) is a potential driver of bias amplification in text-to-image (T2I) diffusion models. We analyzed the magnitude and pattern of CFG-induced bias amplification."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kyungminkim959595.github.io/blog/2025/cfg-bias/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?",
            "description": "Classifier-Free Guidance (CFG) is a potential driver of bias amplification in text-to-image (T2I) diffusion models. We analyzed the magnitude and pattern of CFG-induced bias amplification.",
            "published": "May 27, 2025",
            "authors": [
              
              {
                "author": "Myeongsoo Kim",
                "authorURL": "https://ime.postech.ac.kr/",
                "affiliations": [
                  {
                    "name": "Industrial Management and Engineering, POSTECH",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Kyungmin Kim",
                "authorURL": "https://ai.postech.ac.kr/",
                "affiliations": [
                  {
                    "name": "Graduate School of Artificial Intelligence, POSTECH",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Efficient ML Systems (EECE695E) Blog Post </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?</h1> <p>Classifier-Free Guidance (CFG) is a potential driver of bias amplification in text-to-image (T2I) diffusion models. We analyzed the magnitude and pattern of CFG-induced bias amplification.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <ul> <li> <a href="#motivation">Motivation</a> </li> </ul> <div> <a href="#preliminary">Preliminary</a> </div> <ul> <li> <a href="#group-fairness-and-bias-amplification">Group Fairness and Bias amplification</a> </li> <li> <a href="#diffusion-model-and-classifier-free-guidance">Diffusion Model and Classifier-free Guidance</a> </li> </ul> <div> <a href="#experiment">Experiment</a> </div> <ul> <li> <a href="#t2i-diffusion-models">T2I Diffusion Models</a> </li> <li> <a href="#class-conditional-diffusion-models">Class-conditional Diffusion Models</a> </li> </ul> <div> <a href="#discussion">Discussion</a> </div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <h3 id="motivation">Motivation</h3> <p>Text-to-image (T2I) diffusion models have remarkable capabilities in generating realistic images from input text <d-cite key="seshadri2024bias"></d-cite>. However, there are growing concerns about their tendency to produce biased images toward specific demographic groups (e.g. gender, race, age). While it is inevitable that diffusion models tend to generate images biased toward specific demographic groups due to the societal biases encoded in the large-scale training dataset, they often exacerbate such biases, as illustrated in Figure 1.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/ba_figure-480.webp 480w,/assets/img/cfg_ba/ba_figure-800.webp 800w,/assets/img/cfg_ba/ba_figure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/ba_figure.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" title="Figure 1. Case of bias amplification in Text-to-Image generation (Seshadri et al., 2024)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1. A case of bias amplification in T2I diffusion models <d-cite key="seshadri2024bias"></d-cite>. </div> <p>Which is a factor that drives bias amplification in T2I diffusion models? We pinpoint classifier-free guidance (CFG)—universally applied during T2I diffusion model generation—as a potential contributor. While CFG substantially improves image fidelity and alignment with the conditioning prompt, elevating its strength inherently diminishes sample diversity and biases generation toward prototypical outputs. In practice, users typically operate T2I models with high guidance scales to obtain their desired images algined with prompts.</p> <p>Recently, <d-cite key="kim2024rethinking"></d-cite> empirically demonstrated that higher CFG scales correspond to an increased prevalence of majority-group attributes, thereby identifying CFG intensity as a direct mechanism of bias amplification.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/cfg_ba-480.webp 480w,/assets/img/cfg_ba/cfg_ba-800.webp 800w,/assets/img/cfg_ba/cfg_ba-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/cfg_ba.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" title="Figure 2. Source from Kim et al. (2024)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2. Effect of guidance scale on bias amplification <d-cite key="kim2024rethinking"></d-cite>. </div> <p>In this blog post, we analyze how CFG drives bias amplification in T2I diffusion models. Our blog post is structured as follows: (1) we formulate the bias amplification problem based on existing works investigating the impact of CFG on model distributions; (2) we quantitatively assess the degree to which CFG contributes to bias amplification; and (3) we characterize the relationship between guidance scale and the trend of bias. Unlike a bias mitigation solution that attenuates biases in the latent features of the input prompts from the previous study that identified the bias amplification problem and conducted similar analyses <d-cite key="kim2024rethinking"></d-cite>, we have viewed the problem from a slightly different angle: we frame it as the disparity in bias levels for the attribute between the conditional distribution and the unconditional distribution. We have conducted an experiment on the benchmark for the evaluation of gender–occupation bias, one of the main benchmarks in assessing the fairness of CFG diffusion models.</p> <h2 id="preliminary">Preliminary</h2> <h3 id="group-fairness-and-bias-amplification">Group Fairness and Bias amplification</h3> <h4 id="group-fairness-in-generative-models">Group Fairness in Generative Models</h4> <p>In the context of generative models, group fairness generally refers to ensuring that the model’s outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition:</p> \[\mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right) \approx \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),\] <p>for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model’s output distribution is not biased toward any specific sub-population.</p> <h4 id="bias-amplification-in-generative-models">Bias Amplification in Generative Models</h4> <p>Bias amplification is the phenomenon where a model’s outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.</p> <h3 id="diffusion-model-and-classifier-free-guidance">Diffusion Model and Classifier-free Guidance</h3> <p>Recent diffusion models belong to the family of score-based generative models and are formulated within the stochastic differential equation (SDE) framework.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/sde-480.webp 480w,/assets/img/cfg_ba/sde-800.webp 800w,/assets/img/cfg_ba/sde-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/sde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 3. Visualization of Diffusion processes. Source from Song et al. (2021)" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3. Visualization of diffusion processes <d-cite key="songscore"></d-cite>. </div> <p>The forward process refers to perturbing the data distribution via a diffusion process. As indicated by the symmetric coloring of the density-representing boxes in the Figure 3, the reverse process reconstructs the marginal distribution of the forward process at each time step. To generate images, one must solve the reverse SDE, which therefore requires an accurate estimate of the score function.</p> <p>Meanwhile, CFG is a sampling strategy that employs an extrapolated score toward the conditional distribution, controlled by the guidance scale \(w\). It pushes samples in the direction of higher \(p(y \mid x)\), but it does not directly correspond to the distribution \(p_w\).</p> <p>Here, it is important to note that the sampling distribution has deviated from the original data distribution. By applying CFG, the intended sampling distribution is inevitably shifted from the pure data distribution to the one weighted by both \(p(y∣x)\) and the guidance scale \(w\), which is a potential cause of bias amplification.</p> <p>Indeed, we have empirically verified in a simulation setup that this such shift amplifies bias in gender-occupation fairness (Figure 4).</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/gmm-480.webp 480w,/assets/img/cfg_ba/gmm-800.webp 800w,/assets/img/cfg_ba/gmm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/gmm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 4. Visualization of bias amplification in CFG" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4. Visualization of bias amplification in CFG. </div> <p>In the following <strong>Experiment</strong> section, we empirically demonstrate that this tendency occurs in real world datasets.</p> <hr> <h2 id="experiment">Experiment</h2> <h3 id="t2i-diffusion-models">T2I Diffusion Models</h3> <h4 id="setting">Setting</h4> <p>We have first verified our hypothesis using the experimental setup from <d-cite key="seshadri2024bias"></d-cite>. Specifically, we generate a set of images with Stable Diffusion v1.5 conditioned on prompts that include each target occupation. We then use a CLIP-based classifier to measure the gender distribution of generated images for each occupation, and compare it with the one measured from LAION training data. We have conducted this experiment repeatedly by varying the guidance scale from 1 to 15. The following is the list of occuptations that we have considered.</p> <ul> <li>Occupation list : <strong>Skewed to male</strong> (‘manager’, ‘scientist’, ‘poet’), <strong>balanced</strong> (‘dentist’, ‘painter’, ‘pharmacist’), <strong>skewed to female</strong> (‘dancer’, ‘hairdresser’, ‘pharmacist’)</li> </ul> <p>Bias amplification measure for each occupation \(y\) is defined as follows:</p> \[BA_w(y) = | P_w (S = female | y) -0.5 | - | T (S = female | y) -0.5 |,\] <p>where $T(\cdot)$ denotes the data distribution and $P_w(\cdot)$ the model distribution with guidance scale $w$. Additionally, the degree of bias amplification by selecting the scale parameter \(w\) is defined as follows:</p> \[BA_w(y) - BA_0(y).\] <h4 id="gender-distribution-and-bias-amplification">Gender Distribution and Bias Amplification</h4> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/sd_plot1-480.webp 480w,/assets/img/cfg_ba/sd_plot1-800.webp 800w,/assets/img/cfg_ba/sd_plot1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/sd_plot1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 5. Female ratio per guidance scale" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/sd_plot2-480.webp 480w,/assets/img/cfg_ba/sd_plot2-800.webp 800w,/assets/img/cfg_ba/sd_plot2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/sd_plot2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 6. Bias amplification per guidance scale" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5. (Left) Proportion of females by guidance scale. (Right) Degree of bias amplification by guidance scale </div> <p>In Figures 5, we observe that for certain occupations, increasing the guidance scale drives the gender ratio to deviate from from 0.5, showing bias amplification.</p> <p>Furthermore, in Figure 5, using a baseline female ratio of approximately 0.3–0.4 as a reference, we find that occupations with higher initial female proportions tend to see further increases in female representation as the guidance scale grows, whereas those with lower initial female proportions experience declines. We set the baseline female ratio according to the marginal female proportion in the distribution of Stable Diffusion model reported by the reference study <d-cite key="cho2023dall"></d-cite>.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/dalleval-480.webp 480w,/assets/img/cfg_ba/dalleval-800.webp 800w,/assets/img/cfg_ba/dalleval-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/dalleval.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Figure 7. Gender ratio reported in Cho et al. (2023), -0.42 maps to 0.34 on a 0–1 scale." loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6. Initial gender ratio reported in <d-cite key="cho2023dall"></d-cite>. Note that -0.42 maps to 0.34 on a 0–1 scale. </div> <h4 id="bias-amplifaction-through-guidance">Bias Amplifaction through Guidance</h4> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/sd_plot3-480.webp 480w,/assets/img/cfg_ba/sd_plot3-800.webp 800w,/assets/img/cfg_ba/sd_plot3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/sd_plot3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 8. Guidance-induced bias amplification per guidance scales" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7. Degree of bias amplification by varying guidance scales. </div> <p>To quantify the extent to which bias is amplified relative to the unconditional model’s gender distribution (female ratio = 0.34), we plotted Figure 7 which visualizes \(BA_w(y) - BA_0(y)\) by varying \(w\) for each occuptation \(y\). We observe that \(BA_w(y) - BA_0(y)\) are positive for most occupations and tend to increase as the guidance scale grows.</p> <h3 id="class-conditional-diffusion-models">Class-conditional Diffusion Models</h3> <p>In this section, we replicate and validate the bias amplification trends observed in T2I diffusion models also occur in small-scale, class-conditional diffusion models. We have observed this phenomenon in a toy gender–occupation bias benchmark by varying an initial gender ratio. Specifically, we have utilized Colored MNIST dataset as a gender-occupation bias benchmark, by vewing digits as occupations and colors as genders.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/te_dt-480.webp 480w,/assets/img/cfg_ba/te_dt-800.webp 800w,/assets/img/cfg_ba/te_dt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/te_dt.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" title="Figure 9. Color ratio per digit" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 8. Initial color ratio per digit. </div> <p>When we have examined the bias amplification trend using a class-conditional diffusion model trained on the Colored MNIST dataset via the same procedure as above, the results show clear amplification from the unconditional color ratio of 5:5. As expected, digits with imbalanced red and green proportions produce stronger amplification.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/te_plot1-480.webp 480w,/assets/img/cfg_ba/te_plot1-800.webp 800w,/assets/img/cfg_ba/te_plot1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/te_plot1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 10. Red ratio per guidance scale, CFG experiment" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 9. Degree of bias amplification by varying guidance scale. </div> <p>We have observed the similar pattern in classifier guidance (CG) diffusion model. This pattern is also observed when we reverse the color ratios: the direction of the bias amplification also reverses. This illustrates that the conditional distribution \(p(y \mid x)\) used for guidance exerts a significant influence on the distribution of guided diffusion models, depending on the guidance scale.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/te_plot2-480.webp 480w,/assets/img/cfg_ba/te_plot2-800.webp 800w,/assets/img/cfg_ba/te_plot2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/te_plot2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Figure 11. Red ratio per guidance scale with classifier trained on original data, CG experiment" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/te_plot3-480.webp 480w,/assets/img/cfg_ba/te_plot3-800.webp 800w,/assets/img/cfg_ba/te_plot3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/cfg_ba/te_plot3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 10. (Left) Degree of bias amplification in CG diffusion model by varying guidance scale with a classifier trained on the original data. (Right) Degree of bias amplification in CG diffusion model by varying guidance scale with a classifier trained on the data, where color ratios are inverted for each digit. </div> <hr> <h2 id="discussion">Discussion</h2> <p>In this blog post, we empirically assess the impact of CFG on bias amplification in T2I diffusion models, demonstrating that CFG amplifies bias by extrapolating the conditional distribution from the unconditional distribution.</p> <p>First, the limitations not addressed in this post are as follows:</p> <ul> <li>The bias evaluation may be precise, since our evaluation operates on high-dimensional, complex text-conditioned distributions.</li> <li>Low guidance scales often generates low fidelity images, making the bias evaluation less reliable. Since we have assigned gender by mapping each image to the closest class using a CLIP classifier, the CLIP model’s own biases may have influenced the results. To address these, we have additionally validated the observed trends on a toy experiment using the Colored MNIST dataset.</li> </ul> <p>The contributions of our blog post can be summarized as follows:</p> <ul> <li>In most of the papers we have reviewed so far, there is little or no discussion of CFG’s impact on bias amplification, often missing their experimental set-ups and detailed analysis.</li> <li>From the previous work that identified the bias amplification problem and conducted similar analyses <d-cite key="kim2024rethinking"></d-cite>, they proposed a bias mitigation solution that attenuates biases in the latent features of the input prompts. We have viewed the problem from a slightly different angle, framing it as the disparity in bias levels for the attribute between the conditional distribution and the unconditional distribution.</li> <li>Our future research direction is to reduce this disparity, and we believe that one promising direction is to leverage Autoguidance—one of the recent state-of-the-art CFG methods. The methodology leverages the conditional distribution of a weaker model rather than the unconditional distribution. However, it still does not resolve which weaker model should be employed within T2I diffusion frameworks, necessitating further research.</li> </ul> <p>We believe that group-fairness research for diffusion models must address the effect of CFG, and argue that debiasing low-temperature sampling strategies is essential for building fair generative models with high fidelity.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-05-27-cfg-bias.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>