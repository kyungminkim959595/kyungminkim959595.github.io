<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kyungminkim959595.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kyungminkim959595.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-28T06:02:37+00:00</updated><id>https://kyungminkim959595.github.io/feed.xml</id><title type="html">Efficient ML Systems (EECE695E) Blog Post</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?</title><link href="https://kyungminkim959595.github.io/blog/2025/cfg-bias/" rel="alternate" type="text/html" title="How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://kyungminkim959595.github.io/blog/2025/cfg-bias</id><content type="html" xml:base="https://kyungminkim959595.github.io/blog/2025/cfg-bias/"><![CDATA[<h2 id="group-fairness-and-bias-amplification-in-generative-models">Group Fairness and Bias Amplification in Generative Models</h2> <h3 id="group-fairness-in-generative-models">Group Fairness in Generative Models</h3> <p>In the context of generative models, group fairness generally refers to ensuring that the model’s outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition:</p> \[\mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right) \approx \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),\] <p>for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model’s output distribution is not biased toward any specific sub-population.</p> <h3 id="bias-amplification-in-generative-models">Bias Amplification in Generative Models</h3> <p>Bias amplification is the phenomenon where a model’s outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.</p> <h3 id="demographic-bias-in-text-to-image-diffusion-models">Demographic Bias in Text-to-image Diffusion Models</h3> <p>There has been a surge of empirical findings demonstrating demographic bias in text-to-image diffusion models. Most studies curate and utilize diagnostic prompts to systmetically analyze bias in state-of-the-art models such as DALL-E and Stable Diffusion. Although the specific image generation task and the protected attribute may vary, the majority of studies aim to determine whether bias persists even when neutral prompts are used. Specifically, they examine whether certain occupations or adjectives are more freqeuntly associated with sub-populations defined by demographic attributes. For example, <d-cite key="wu2024stable"></d-cite> showed that prompts containing positive adjectives are more likely to generate images of lighter skin subjects, while prompts containing negative attributes tend to produce images of darker-skinned subjects. In addition to such distributional bias across different demographic groups, bias can also be perpetuated in terms of generation diversity: <d-cite key="aldahoul2025ai"></d-cite> showed that Stable Diffusion XL tends to generate similar-looking faces for minority groups, without catching any details and nuances of their facial expressions.</p> <h3 id="bias-evaluation-in-text-to-image-diffusion-models">Bias Evaluation in Text-to-image Diffusion Models</h3> <p>In addition to the curation of the diagnostic prompts, it is essential to annotate each generated image with both its corresponding label and associated protected attribute. To evaluate the bias of a given text-to-image diffusion model, we first generate a large sample of images for each of the prompt. For example, in the occupation image generation task, we prompt the model with $``\text{Image of a} ~ \texttt{{ job }}’’$ to generate multiple samples. Then, either a human annotator or a model is used to annotate the protected attribute for each image in order to evaluate group fairness. While the literature still lacks a unified benchmark and evaluation framework for assessing bias in text-to-image diffusion models, existing studies consistently report that such bias exists and must be mitigated to ensure the trustworthy use of these models.</p> <hr/> <h2 id="bias-amplification-in-classifier-free-guidance-for-diffusion-models">Bias Amplification in Classifier-free Guidance for Diffusion Models</h2> <hr/>]]></content><author><name>Myeongsoo Kim</name></author><category term="CFG"/><category term="diffusion_model"/><category term="bias"/><summary type="html"><![CDATA[Literature review and empirical analyses on the bias amplification in CFG for diffusion models]]></summary></entry></feed>