<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kyungminkim959595.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kyungminkim959595.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-30T08:19:25+00:00</updated><id>https://kyungminkim959595.github.io/feed.xml</id><title type="html">Efficient ML Systems (EECE695E) Blog Post</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?</title><link href="https://kyungminkim959595.github.io/blog/2025/cfg-bias/" rel="alternate" type="text/html" title="How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://kyungminkim959595.github.io/blog/2025/cfg-bias</id><content type="html" xml:base="https://kyungminkim959595.github.io/blog/2025/cfg-bias/"><![CDATA[<h2 id="introduction">Introduction</h2> <h3 id="motivation">Motivation</h3> <p>Text-to-image (T2I) diffusion models have remarkable capabilities in generating realistic images from input text. (Naik &amp; Nushi, 2023) However, there are growing concerns about their tendency to produce biased images toward specific demographic groups ex) gender bias, skin tone bias, attribute bias, …. Because large-scale datasets encode these social biases and diffusion models learn from that distribution, they tend to generate biased images toward specific demographic groups. We address the bias amplification phenomenon such that a model’s outputs exhibit stronger biases than those present in its training data like figure 1.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cfg_ba/ba_figure-480.webp 480w,/assets/img/cfg_ba/ba_figure-800.webp 800w,/assets/img/cfg_ba/ba_figure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cfg_ba/ba_figure.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="여기에 캡션을 입력하세요" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Which factors drive bias amplification in T2I diffusion models? We pinpoint Classifier-Free Guidance (CFG)—universally applied during T2I diffusion model generation—as a potential contributor. While CFG substantially improves image fidelity and alignment with the conditioning prompt, elevating its strength inherently diminishes sample diversity and biases generation toward prototypical outputs. In practice, users typically operate in the higher guidance‐scale range, adjusting the parameter to obtain their desired images.</p> <p>Recently, Kim et al. (2024) empirically demonstrated that higher CFG scales correspond to an increased prevalence of majority-group attributes, thereby identifying CFG intensity as a direct mechanism of bias amplification.</p> <p>\figure 2</p> <p>In this blog post, we analyze how Classifier‐Free Guidance (CFG) drives bias amplification in text‐to‐image diffusion models. Our work proceeds in three steps: (1) we formulate the problem by surveying prior research on CFG’s impact on model distributions; (2) we quantitatively assess the degree to which CFG contributes to bias amplification; and (3) we characterize the relationship between guidance scale and bias trends. We conducted our experimental analysis on the gender–occupation bias, a primary focus in fairness studies of text-to-image generative models.</p> <table> <tbody> <tr> <td>Based on these findings, we argue that—unlike mainstream efforts aimed at enforcing fairness in the conditional distribution p(x</td> <td>y) —future research must target fairness in the reverse conditional p(y</td> <td>x), particularly under low‐temperature sampling regimes.</td> </tr> </tbody> </table> <hr/> <h2 id="preliminary">Preliminary</h2> <h3 id="group-fairness-and-bias-amplification">Group Fairness and Bias amplification</h3> <h3 id="diffusion-model-and-classifier-free-guidance">Diffusion Model and Classifier-free guidance</h3> <p>Recent diffusion models belong to the family of score-based generative models and are formulated within the stochastic differential equation (SDE) framework.</p> <p>\figure 3</p> <p>The forward process refers to perturbing the data distribution via a diffusion process. As indicated by the symmetric coloring of the density-representing boxes in the figure, the reverse process reconstructs the marginal distributions of the forward process at each time step. To generate images, one must solve the reverse SDE, which therefore requires an accurate estimate of the score function.</p> <p>Classifier-free guidance(CFG) is a sampling strategy that employs an extrapolated score toward the conditional distribution, controlled by the guidance scale w, as follows. It pushes samples in the direction of higher p(c \mid x), but it does not directly correspond to the distribution p_w.</p> \[p\] <p>Here, it is important to note that the sampling distribution has deviated from the original data distribution. Bias amplification refers to the phenomenon in which a diffusion model’s sample distribution—expected in theory to follow the data distribution—actually exhibits an exacerbated bias. By applying CFG, however, the intended sampling distribution is inherently shifted from the pure data distribution to one weighted by both p(y∣x) and the guidance scale w.</p> <p>Furthermore, we demonstrated in a simple setup (see Figure 4) that this shift acts to amplify bias in gender-occupation fairness. While the unconditional distribution corresponds to a single marginal distribution, the conditional distribution can exhibit higher or lower densities for specific groups depending on the conditioning variable y.</p> <p>\figure 3</p> <p>In the Experiments section, we empirically demonstrate that this tendency can indeed be observed in both text-to-image diffusion models and a toy example.</p> <hr/> <h2 id="experiment">Experiment</h2> <h3 id="t2i-diffusion-model--stable-diffusion">T2I diffusion model : Stable diffusion</h3> <h3 id="class-conditional-diffusion-model">Class conditional diffusion model</h3> <hr/> <h2 id="discussion">Discussion</h2> <p>In this blog post, we empirically assessed the impact of CFG on bias amplification in T2I diffusion models, demonstrating that CFG amplifies bias by extrapolating</p> <p>the conditional distribution further away from the unconditional distribution.</p> <p>The limitations not addressed in this post are as follows:</p> <ul> <li>The numerical values cannot be considered strictly precise because our evaluation operates on high-dimensional, complex text-conditioned distributions.</li> <li>Images generated at low guidance scales often fail to form properly, making those measurements less reliable. Since we assigned gender by mapping each image to the closest class using a CLIP classifier, the CLIP model’s own biases may have influenced our results.</li> </ul> <p>To address these, we validated the observed trends using toy examples.</p> <p>Through this blog post, we emphasize that group-fairness research for diffusion models must address the effects of CFG, and more broadly, that debiasing low-temperature sampling strategies is essential for generative models.</p> <ul> <li>In most of the papers I reviewed, there is little to no discussion of CFG’s impact, and their experimental setups are often missed.</li> <li>Kim et al. (2024), who previously identified the same problem, proposed a solution that attenuates bias in the text-condition embeddings of the text-conditional component. We approached the problem from a slightly different angle, framing it as the disparity in bias levels for the attribute between the conditional distribution and the unconditional distribution.</li> <li>Our proposal is to reduce this disparity, and we believe that one promising direction is to leverage Autoguidance—one of the recent state-of-the-art CFG methods. This methodology leverages the conditional distribution of a weaker model rather than the unconditional distribution. However, it still does not resolve which weaker model should be employed within T2I diffusion frameworks, necessitating further research.</li> </ul> <p>We have always considered fairness only with respect to p(x∣y)p(x \mid y), but for practical applications, fairness in p(y∣x)p(y \mid x) must also be taken into account.</p> <hr/>]]></content><author><name>Myeongsoo Kim</name></author><category term="CFG"/><category term="diffusion_model"/><category term="bias"/><summary type="html"><![CDATA[Literature review and empirical analyses on the bias amplification in CFG for diffusion models]]></summary></entry><entry><title type="html">How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?</title><link href="https://kyungminkim959595.github.io/blog/2025/cfg-bias_backup/" rel="alternate" type="text/html" title="How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://kyungminkim959595.github.io/blog/2025/cfg-bias_backup</id><content type="html" xml:base="https://kyungminkim959595.github.io/blog/2025/cfg-bias_backup/"><![CDATA[<h2 id="group-fairness-and-bias-amplification-in-generative-models">Group Fairness and Bias Amplification in Generative Models</h2> <h3 id="group-fairness-in-generative-models">Group Fairness in Generative Models</h3> <p>In the context of generative models, group fairness generally refers to ensuring that the model’s outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition:</p> \[\mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right) \approx \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),\] <p>for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model’s output distribution is not biased toward any specific sub-population.</p> <h3 id="bias-amplification-in-generative-models">Bias Amplification in Generative Models</h3> <p>Bias amplification is the phenomenon where a model’s outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.</p> <h3 id="demographic-bias-in-text-to-image-diffusion-models">Demographic Bias in Text-to-image Diffusion Models</h3> <p>There has been a surge of empirical findings demonstrating demographic bias in text-to-image diffusion models. Most studies curate and utilize diagnostic prompts to systmetically analyze bias in state-of-the-art models such as DALL-E and Stable Diffusion. Although the specific image generation task and the protected attribute may vary, the majority of studies aim to determine whether bias persists even when neutral prompts are used. Specifically, they examine whether certain occupations or adjectives are more freqeuntly associated with sub-populations defined by demographic attributes. For example, <d-cite key="wu2024stable"></d-cite> showed that prompts containing positive adjectives are more likely to generate images of lighter skin subjects, while prompts containing negative attributes tend to produce images of darker-skinned subjects. In addition to such distributional bias across different demographic groups, bias can also be perpetuated in terms of generation diversity: <d-cite key="aldahoul2025ai"></d-cite> showed that Stable Diffusion XL tends to generate similar-looking faces for minority groups, without catching any details and nuances of their facial expressions.</p> <h3 id="bias-evaluation-in-text-to-image-diffusion-models">Bias Evaluation in Text-to-image Diffusion Models</h3> <p>To assess bias in a text-to-image diffusion model, we begin by generating a large sample of images for each prompt. For example, in the occupation image generation task, we prompt the model with “$\text{Image of a} ~ \texttt{{job}}$” to generate multiple samples. Then, either a human annotator or a model is used to annotate the protected attribute of each image, allowing for the evaluation of group fairness. Demographic parity across gender, for instance, can then be measured by computing the proportion of male and female images generated for each occupation. Although there is still no unified benchmark and evaluation framework for bias assessment in text-to-image diffusion models, existing studies consistently report that such biases exist and must be mitigated to ensure the trustworthy use of these models.</p> <hr/> <h2 id="bias-amplification-in-classifier-free-guidance-for-diffusion-models">Bias Amplification in Classifier-free Guidance for Diffusion Models</h2> <p>While the classifier-free guidance in image generation helps text-to-image diffusion models produce images that more faithfully align with the prompt semantics, this alignment can also lead to bias amplification. Specifically, the guidance scale can be seen as a double-edged sword, as it not only aligns the output more closely with the prompt, but also acts as an amplifier of the biases the model has learned from its training data. Recently, <d-cite key="kim2024rethinking"></d-cite> empirically demonstrated how varying the guidance scale impacts bias amplification. Lower guidance tends to yield more balanced outputs that better reflect the distribution in the training data, albeit with relatively low fidelity. Meanwhile, <d-cite key="kim2024rethinking"></d-cite> has additionally reported that it is much more feasible to produce minority outcomes from the latent space, which could have been difficult to sample in the original space. While several studies have explored ways to mitigate bias in classifier-free guidance, such as prepending fairness prompts <d-cite key="friedrich2023fair"></d-cite> or applying fine-tuning <d-cite key="shen2024finetuning"></d-cite>, we focus on the detailed analysis of how the guidance scale affects the bias amplification on text-to-image diffusion models in subsequent sections.</p> <hr/>]]></content><author><name>Myeongsoo Kim</name></author><category term="CFG"/><category term="diffusion_model"/><category term="bias"/><summary type="html"><![CDATA[Literature review and empirical analyses on the bias amplification in CFG for diffusion models]]></summary></entry></feed>