<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://kyungminkim959595.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kyungminkim959595.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-29T01:30:16+00:00</updated><id>https://kyungminkim959595.github.io/feed.xml</id><title type="html">Efficient ML Systems (EECE695E) Blog Post</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?</title><link href="https://kyungminkim959595.github.io/blog/2025/cfg-bias/" rel="alternate" type="text/html" title="How Does Classifier-free Guidance Amplify the Societal Bias in Text-to-image Diffusion Models?"/><published>2025-05-27T00:00:00+00:00</published><updated>2025-05-27T00:00:00+00:00</updated><id>https://kyungminkim959595.github.io/blog/2025/cfg-bias</id><content type="html" xml:base="https://kyungminkim959595.github.io/blog/2025/cfg-bias/"><![CDATA[<h2 id="group-fairness-and-bias-amplification-in-generative-models">Group Fairness and Bias Amplification in Generative Models</h2> <h3 id="group-fairness-in-generative-models">Group Fairness in Generative Models</h3> <p>In the context of generative models, group fairness generally refers to ensuring that the model’s outputs are demographically balanced across different protected attributes (e.g. gender, race, age). Traditional fairness metrics from supervised learning, such as demographic parity, are adopted to evaluate the group fairness of generative models. For example, in the occupation image generation task, we say that a generative model is gender-fair if it satisfies the following condition:</p> \[\mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is female} \right) \approx \mathbb{P}\left( \text{Image of a} ~ \{ \texttt{job} \} ~ | ~ \text{Subject is male} \right),\] <p>for every job candidate<d-footnote>Note that additional annotation for both the image label and the protected attribute is necessary for each generated image.</d-footnote>. While there is no universal definition of fairness <d-cite key="barocas2023fairness"></d-cite>, group fairness generally refers to a scenario in which the generative model’s output distribution is not biased toward any specific sub-population.</p> <h3 id="bias-amplification-in-generative-models">Bias Amplification in Generative Models</h3> <p>Bias amplification is the phenomenon where a model’s outputs exhibit stronger biases than those present in its training data <d-cite key="seshadri2024bias"></d-cite>. While such phenomena have been observed in text-to-image diffusion models such as Stable Diffusion, <d-cite key="seshadri2024bias"></d-cite> empirically demonstrated that in some cases, this may be due to prompt distribution shift-a mismatch between the captions used during training and the prompts provided at inference time. This finding highlights the importance of carefully evaluating bias amplification. Nevertheless, there is consensus that bias still persists in text-to-image diffusion models.</p> <h3 id="demographic-bias-in-text-to-image-diffusion-models">Demographic Bias in Text-to-image Diffusion Models</h3> <p>There has been a surge of empirical findings demonstrating demographic bias in text-to-image diffusion models. Most studies curate and utilize diagnostic prompts to systmetically analyze bias in state-of-the-art models such as DALL-E and Stable Diffusion. Although the specific image generation task and the protected attribute may vary, the majority of studies aim to determine whether bias persists even when neutral prompts are used. Specifically, they examine whether certain occupations or adjectives are more freqeuntly associated with sub-populations defined by demographic attributes. For example, <d-cite key="wu2024stable"></d-cite> showed that prompts containing positive adjectives are more likely to generate images of lighter skin subjects, while prompts containing negative attributes tend to produce images of darker-skinned subjects. In addition to such distributional bias across different demographic groups, bias can also be perpetuated in terms of generation diversity: <d-cite key="aldahoul2025ai"></d-cite> showed that Stable Diffusion XL tends to generate similar-looking faces for minority groups, without catching any details and nuances of their facial expressions.</p> <h3 id="bias-evaluation-in-text-to-image-diffusion-models">Bias Evaluation in Text-to-image Diffusion Models</h3> <p>In addition to curation of the diagnostic prompts, it is crucial to annotate each generated image with both its corresponding label and protected attribute. To assess bias in a text-to-image diffusion model, we begin by generating a large sample of images for each prompt. For example, in the occupation image generation task, we prompt the model with “$\text{Image of a} ~ \texttt{{job}}$” to generate multiple samples. Then, either a human annotator or a model is used to annotate the protected attribute of each image, allowing for the evaluation of group fairness. Demographic parity across gender, for instance, can then be measured by computing the proportion of male and female images generated for each occupation. Although there is still no unified benchmark and evaluation framework for bias assessment in text-to-image diffusion models, existing studies consistently report that such biases exist and must be mitigated to ensure the trustworthy use of these models.</p> <hr/> <h2 id="bias-amplification-in-classifier-free-guidance-for-diffusion-models">Bias Amplification in Classifier-free Guidance for Diffusion Models</h2> <p>While the classifier-free guidance in image generation helps text-to-image diffusion models produce images that more faithfully align with the prompt semantics, this alignment can also lead to bias amplification. Specifically, the guidance scale can be seen as a double-edged sword, as it not only aligns the output more closely with the prompt, but also acts as an amplifier of the biases the model has learned from its training data. Recently, <d-cite key="kim2024rethinking"></d-cite> empirically demonstrated how varying the guidance scale impacts bias amplification. Lower guidance tends to yield more balanced outputs that better reflect the distribution in the training data, albeit with relatively low fidelity. Meanwhile, <d-cite key="kim2024rethinking"></d-cite> has additionally reported that it is much more feasible to produce minority outcomes from the latent space, which could have been difficult to sample in the original space. While several studies have explored ways to mitigate bias in classifier-free guidance, for example, such as prepending fairness prompts <d-cite key="friedrich2023fair"></d-cite> or applying fine-tuning <d-cite key="shen2024finetuning"></d-cite>, we focus on the detailed analysis of how the guidance scale affects the bias amplification on text-to-image diffusion models in subsequent sections.</p> <hr/>]]></content><author><name>Myeongsoo Kim</name></author><category term="CFG"/><category term="diffusion_model"/><category term="bias"/><summary type="html"><![CDATA[Literature review and empirical analyses on the bias amplification in CFG for diffusion models]]></summary></entry></feed>